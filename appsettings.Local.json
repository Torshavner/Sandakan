{
  "server": {
    "host": "127.0.0.1",
    "port": 3000
  },
  "qdrant": {
    "url": "http://localhost:6334",
    "collection_name": "rag_chunks"
  },
  "database": {
    "url": "postgres://sandakan:sandakan@localhost:5432/sandakan",
    "max_connections": 10,
    "run_migrations": true
  },
  "embeddings": {
    "provider": "local",
    "model": "sentence-transformers/all-MiniLM-L6-v2",
    "strategy": "semantic",
    "dimension": 384,
    "chunk_overlap": 50
  },
  "chunking": {
    "max_chunk_size": 512,
    "overlap_tokens": 50
  },
  "llm": {
    "provider": "lmstudio",
    "api_key": "lm-studio",
    "base_url": "http://localhost:1234/v1",
    "chat_model": "lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF",
    "max_tokens": 2048,
    "temperature": 0.7,
    "sse_keep_alive_seconds": 15
  },
  "logging": {
    "level": "debug",
    "enable_json": false,
    "enable_udp": true
  },
  "extraction": {
    "pdf": {
      "enabled": true,
      "max_file_size_mb": 50
    },
    "audio": {
      "enabled": true,
      "max_file_size_mb": 100,
      "whisper_model": "openai/whisper-base",
      "provider": "local"
    },
    "video": {
      "enabled": true,
      "max_file_size_mb": 500
    }
  },
  "rag": {
    "similarity_threshold": 0.7,
    "max_context_tokens": 3072,
    "top_k": 5,
    "system_prompt": "You are a helpful assistant. Answer the user's question using ONLY the provided context. If the context does not contain enough information to answer, say so. Do not use any external knowledge.\n\nContext:\n{context}",
    "fallback_message": "I cannot answer this based on the available lecture notes."
  }
}
