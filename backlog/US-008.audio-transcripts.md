**Filename:** `docs/user_stories/20260212_asynchronous_video_ingestion_pipeline.md`

```markdown
# Asynchronous Video Ingestion Pipeline

## Requirement Definition
As a System Architect, I need a decoupled, job-based ingestion pipeline so that large media files (up to 1-hour) can be processed via Whisper or external APIs without blocking the HTTP runtime or risking system crashes.

## Problem Statement
* **Current bottleneck/technical debt:** Synchronous processing of media files leads to HTTP timeouts and thread starvation in the `tokio` runtime.
* **Performance/cost implications:** Single-threaded transcription blocks the entire ingestion service; lack of progress tracking results in poor UX and "dark" failures.
* **Architectural necessity:** To support scalability and "Hot-Swapping" of inference engines (Local Whisper vs. OpenAI), the system requires a formal `TranscriptionEngine` trait and a background worker pattern.

## Acceptance Criteria (Gherkin Enforced)
### Job Submission & Handover
* **Given** a valid video file path or URL,
* **When** a `POST /api/v1/ingest` request is received,
* **Then** the system returns a `202 Accepted` status with a unique `UUID` job_id and persists the job state as `Pending`.

### Stage-Aware Processing
* **Given** a job in the internal `mpsc` channel,
* **When** the Background Worker processes the file,
* **Then** the `JobStatus` must transition sequentially through `MediaExtraction` -> `Transcribing` -> `Embedding` -> `Completed`.

### Engine Interoperability
* **Given** a `ProcessingConfig` set to `OpenAi` or `LocalWhisper`,
* **When** the `Transcribing` stage is reached,
* **Then** the system invokes the `TranscriptionEngine::process` trait method without modifying the Demuxer or Resampler logic.

* **Technical Metric:** Maximum memory overhead during 16kHz PCM conversion must not exceed 2x the raw audio size.
* **Observability:** Every stage transition must be logged using the `tracing` crate with the `job_id` included in the span context.

## Technical Context
* **Architectural patterns:** Pipes and Filters, Hexagonal Architecture (Ports/Adapters).
* **Stack components:** `axum` (API), `tokio::sync::mpsc` (Queue), `symphonia` (Demuxer), `whisper-rs`/`candle` (Inference).
* **Integration points:** Qdrant (Vector Sink), Local Filesystem / Cloud Storage.
* **Namespace/Config:** `app.ingestion.max_concurrent_jobs`, `app.transcription.provider`.

## Cross-Language Mapping
* `TranscriptionEngine` Trait ≈ Interface (Java/TS) / Abstract Base Class (Python).
* `tokio::spawn` ≈ Background Worker Thread / Goroutine.

## Metadata
* **Dependencies:** None (Foundation Layer)
* **Complexity:** High
* **Reasoning:** Requires robust error handling (catch_unwind), complex async state management across thread boundaries, and precise audio buffer manipulation.

## Quality Benchmarks
## Test-First Development Plan
- [ ] Parse criteria into Given-When-Then scenarios.
- [ ] Generate failing test suite in `tests/ingestion_pipeline.rs`.
- [ ] Execute `cargo test` to confirm 404/Fail on missing endpoints.
- [ ] Implement `TranscriptionEngine` trait and `mpsc` worker loop.
- [ ] Refactor for zero-clone buffer passing and verify green state.

This is a strong engineering choice. For 1-hour videos, you cannot keep an HTTP request open. Moving to a Job-based Background Service is the "production way" to handle this.

```

Would you like me to generate the Rust trait definitions and the `JobStatus` enum for this pipeline?