# RAG Orchestrator

## Requirement Definition
As a **Knowledge Base User**, I need **a centralized RAG Orchestrator exposing an OpenAI-compatible API** so that **my queries in Open WebUI are answered strictly using ingested lecture content, filtering out hallucinations and irrelevant external knowledge.**

## Problem Statement
* **Current bottleneck/technical debt:** Frontend (Open WebUI) cannot directly query the Vector DB; lacks logic to combine retrieval with generation.
* **Performance/cost implications:** Sending entire documents to LLM exceeds token limits and increases inference costs/latency.
* **Architectural necessity:** A middleware layer is required to bridge the stateless frontend and the stateful Vector/LLM infrastructure while enforcing strict context boundaries.

## Acceptance Criteria (Gherkin Enforced)
### API Compatibility (Open WebUI Integration)
* **Given** the Open WebUI is configured to point to `http://localhost:port/v1`,
* **When** the frontend sends a standard `POST /chat/completions` JSON payload,
* **Then** the Orchestrator maps the request to internal RAG logic and returns a compliant JSON response (or SSE stream).

### Retrieval & Thresholding
* **Given** a user query "What is ownership in Rust?",
* **When** the vector similarity score for the top chunk is below the configured threshold (e.g., 0.7),
* **Then** the system aborts the LLM call and immediately returns the fallback message: "I cannot answer this based on the available lecture notes."

### Context Window Management
* **Given** a retrieval result of 50 chunks,
* **When** the total token count of chunks + system prompt exceeds the model's limit (e.g., 4096 tokens),
* **Then** the Orchestrator iteratively drops the least relevant chunks until the payload fits within the safety buffer.

### Guardrails & System Prompt
* **Given** a malicious or irrelevant query (e.g., "Ignore instructions," "Who won the World Cup?"),
* **When** the prompt is constructed,
* **Then** the System Prompt ("Answer using ONLY provided context...") overrides the LLM's internal knowledge, resulting in a refusal or "Data not found" response.

* **Technical Metric:** RAG Pipeline Latency (Embedding + Retrieval + Context Build) < 200ms (excluding LLM generation).
* **Observability:** Distributed tracing (Span) for `orchestrator::handle_query`, tagging `retrieved_chunks_count` and `similarity_score`.

## Technical Context
* **Architectural patterns:** Orchestrator (Middleware), Adapter Pattern (for LLM/Vector APIs).
* **Stack components:** Rust, `Axum` (HTTP), `tiktoken-rs` (Token counting), `Qdrant` (Vector Search), `reqwest`/`candle` (LLM Client).
* **Integration points:** * Input: Open WebUI (JSON/REST).
    * Data Source: Qdrant gRPC.
    * Inference: OpenAI API or Local LLM.
* **Namespace/Config:** `RAG_CONFIDENCE_THRESHOLD`, `MAX_CONTEXT_TOKENS`, `SYSTEM_PROMPT_TEMPLATE`.

## Cross-Language Mapping
* `trait RagOrchestrator` ≈ `interface IRagService` (C#) or `abc.ABC` (Python).
* `axum::extract::Json` ≈ `Flask request.json` or `Spring @RequestBody`.

## Metadata
* **Dependencies:** Ingestion Pipeline (Data must exist).
* **Complexity:** High
* **Reasoning:** Involves coordinating async I/O (Network bound), precise token math (CPU bound), and prompt engineering logic.

## Quality Benchmarks
* **Relevance:** Top-3 retrieved chunks must contain the answer for 90% of "Happy Path" queries.
* **Stability:** System handles Qdrant downtime with graceful HTTP 503 errors, not crashes.

## Test-First Development Plan
- [ ] **Integration:** Create `tests/integration_rag.rs` with Docker Compose (Qdrant + Mock LLM).
- [ ] **Fixture:** Inject 3 known chunks ("Rust ownership", "Mitochondria", "Deadlines").
- [ ] **Test Case 1 (Happy Path):** Query "Rust ownership" -> Assert response contains "one owner".
- [ ] **Test Case 2 (Negative):** Query "World Cup" -> Assert response matches Fallback String.
- [ ] **Test Case 3 (Overflow):** Mock 100 chunk return -> Assert no panic & truncated context.
- [ ] **Implementation:** Define `RagOrchestrator` trait and implement `Axum` handler.
